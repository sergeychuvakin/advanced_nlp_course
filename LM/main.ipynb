{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to run in colab\n",
    "\n",
    "# !git clone https://github.com/sergeychuvakin/advanced_nlp_course.git\n",
    "# !mv advanced_nlp_course/LM/*.py ./\n",
    "# !mv advanced_nlp_course/LM/*.json ./ ## \n",
    "# !pip install loguru pydantic tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "joint-globe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loguru==0.5.3\n",
      "nltk==3.6.2\n",
      "pydantic==1.8.2\n",
      "requests==2.25.1\n",
      "requests-oauthlib==1.3.0\n",
      "tokenizers==0.10.3\n",
      "torch==1.9.0\n",
      "torchtext==0.6.0\n",
      "tqdm==4.59.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | egrep \"pydantic|torch|loguru|tokenizers|requests|nltk|tqdm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "damaged-affect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from loguru import logger\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "from dependencies import corpus, tokenizer\n",
    "from config import Config, LanguageModelConfig\n",
    "from processing_utils import (\n",
    "    clean_text, \n",
    "    split_on_sequences, \n",
    "    create_ngrams, \n",
    "    create_to_x_and_y, \n",
    "    word2int,\n",
    "    create_vocab,\n",
    "    save_artifacts\n",
    ")\n",
    "from model import LM_LSTM\n",
    "from datahandler import LMDataset\n",
    "from train_utils import train_model\n",
    "\n",
    "config = Config()\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-bermuda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = clean_text(corpus)\n",
    "corpus = split_on_sequences(corpus)\n",
    "\n",
    "tcorpus = tokenizer.encode_batch(corpus)\n",
    "\n",
    "## create n-grams for each doc\n",
    "sq = create_ngrams(tcorpus, config.N_GRAM) \n",
    " \n",
    "## shift corpus to create x and y \n",
    "x, y =  create_to_x_and_y(sq)\n",
    "\n",
    "id_token, token_id = create_vocab(tokenizer)\n",
    "vocab_size = len(token_id)\n",
    "\n",
    "## split data\n",
    "tradeoff_index = int(len(x) * config.TRAIN_PROPORTION)\n",
    "\n",
    "x_train = x[:tradeoff_index]\n",
    "x_test = x[tradeoff_index:]\n",
    "\n",
    "y_train = y[:tradeoff_index]\n",
    "y_test = y[tradeoff_index:]\n",
    "\n",
    "logger.warning(\n",
    "    f\"\"\"\n",
    "    Output shapes: \n",
    "        x_train: {len(x_train)}, \n",
    "        x_test: {len(x_test)}, \n",
    "        y_train: {len(y_train)}, \n",
    "        y_test: {len(y_test)}\n",
    "    \"\"\"\n",
    "              )\n",
    "\n",
    "## load to dataset and dataloader\n",
    "train_ds = LMDataset(x_train, y_train)\n",
    "test_ds = LMDataset(x_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# model and model config\n",
    "model_config = LanguageModelConfig(vocab_size=vocab_size, emb_size=300)\n",
    "model = LM_LSTM(**model_config.dict(), logger=logger)\n",
    "\n",
    "## save artifacts\n",
    "save_artifacts(\n",
    "    (model_config.dict(), config.SAVE_MODEL_CONFIG),\n",
    "    (token_id, config.SAVE_TOKEN_ID),\n",
    "    (id_token, config.SAVE_ID_TOKEN)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_config.lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# train model \n",
    "tmodel = train_model(model,\n",
    "                     train_dl,\n",
    "                     optimizer=optimizer,\n",
    "                     loss_func=loss_func,\n",
    "                     epochs=5, \n",
    "                     clip=1)\n",
    "\n",
    "torch.save(model.state_dict(), config.SAVE_MODEL_FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "linear-statistics",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cleared-demonstration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-12 14:35:42.930 | WARNING  | __main__:<module>:6 - \n",
      "    Cross-Entropy: 10.326239\n",
      "    Perpelxity: 30523.087891\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "logger.warning(\n",
    "    \"\"\"\n",
    "    Cross-Entropy: %f\n",
    "    Perpelxity: %f\n",
    "    \"\"\" % \n",
    "    val_metrics(model, test_dl, token_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-nepal",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "rental-polyester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LM_LSTM(\n",
       "  (emb_layer): Embedding(30523, 300)\n",
       "  (lstm): LSTM(300, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=30523, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from config import LanguageModelConfig, Config\n",
    "from model import LM_LSTM\n",
    "from loguru import logger\n",
    "from processing_utils import load_artifact\n",
    "from dependencies import tokenizer\n",
    "\n",
    "## usefull utils\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"WARNING\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## load artifacts\n",
    "model_config = LanguageModelConfig.parse_file(Config.SAVE_MODEL_CONFIG)\n",
    "token_id = load_artifact(Config.SAVE_TOKEN_ID)\n",
    "id_token = load_artifact(Config.SAVE_ID_TOKEN)\n",
    "\n",
    "## load trained model\n",
    "model = LM_LSTM(**model_config.dict(), logger=logger)\n",
    "model.load_state_dict(\n",
    "    torch.load(Config.SAVE_MODEL_FNAME, map_location=device)\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "welcome-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "def _transform_raw_word(word, token_id):\n",
    "    int_word = token_id.get(word, token_id.get(Config.TOKEN_UNKNOWN))\n",
    "    return torch.tensor([[int_word]]).to(device)\n",
    "\n",
    "def _get_model_output(model, input_tensor, hidden_state=None):\n",
    "    if not hidden_state:\n",
    "        return model(input_tensor, model.init_state(1))\n",
    "    else:\n",
    "        return model(input_tensor, hidden_state)\n",
    "\n",
    "def _transform_model_output(out, random, id_token, top=3):\n",
    "    \n",
    "    if random:\n",
    "        idx = sample(list(torch.topk(out, 3).indices[0]), 1)[0].item()\n",
    "        \n",
    "    else:\n",
    "        idx = torch.topk(out, top).indices[0][0].item()\n",
    "    return id_token[str(idx)]\n",
    "\n",
    "    \n",
    "\n",
    "def predict_one_word(\n",
    "    word, \n",
    "    model, \n",
    "    token_id, \n",
    "    id_token,\n",
    "    random=True\n",
    "):\n",
    "    \n",
    "    input_tensor = _transform_raw_word(word, token_id)\n",
    "    out, h = _get_model_output(model, input_tensor)\n",
    "    return _transform_model_output(out, random, id_token)\n",
    "\n",
    "def predict_sample(\n",
    "    word, \n",
    "    model, \n",
    "    token_id, \n",
    "    id_token,\n",
    "    length_of_sample, \n",
    "    random=True\n",
    "):\n",
    "    result = []\n",
    "    h = None\n",
    "    while len(result) < length_of_sample:\n",
    "        \n",
    "        input_tensor = _transform_raw_word(word, token_id)\n",
    "        word, h = _get_model_output(model, input_tensor, h)\n",
    "        result.append(_transform_model_output(word, random, id_token))\n",
    "    return \" \".join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "radio-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_one_word(\n",
    "    \"one of the\",  \n",
    "    model, \n",
    "    token_id, \n",
    "    id_token,\n",
    "    random=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "southwest-lebanon",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. the , . , , , the , the'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sample(\n",
    "    \"one\", \n",
    "    model, \n",
    "    token_id, \n",
    "    id_token,\n",
    "    10, \n",
    "    random=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
