{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to run in colab\n",
    "\n",
    "# !git clone https://github.com/sergeychuvakin/advanced_nlp_course.git\n",
    "# !mv advanced_nlp_course/LM/*.py ./\n",
    "# !pip install loguru pydantic tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "smoking-bailey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loguru==0.5.3\n",
      "nltk==3.6.2\n",
      "pydantic==1.8.2\n",
      "requests==2.25.1\n",
      "requests-oauthlib==1.3.0\n",
      "tokenizers==0.10.3\n",
      "torch==1.9.0\n",
      "torchtext==0.6.0\n",
      "tqdm==4.59.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze | egrep \"pydantic|torch|loguru|tokenizers|requests|nltk|tqdm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "internal-kansas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from loguru import logger\n",
    "import sys\n",
    "import json\n",
    "\n",
    "\n",
    "from dependencies import corpus, tokenizer\n",
    "from config import Config, LanguageModelConfig\n",
    "from processing_utils import (\n",
    "    clean_text, \n",
    "    split_on_sequences, \n",
    "    create_ngrams, \n",
    "    create_to_x_and_y, \n",
    "    word2int,\n",
    "    create_vocab,\n",
    "    save_artifacts\n",
    ")\n",
    "from model import LM_LSTM\n",
    "from datahandler import LMDataset\n",
    "from train_utils import train_model\n",
    "\n",
    "config = Config()\n",
    "\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"WARNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-password",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52810/52810 [00:00<00:00, 124484.61it/s]\n",
      "2021-11-11 23:26:35.119 | WARNING  | __main__:<module>:29 - Outpur shapes: x_train: 969530, x_test: 157831, y_train: 969530, y_test: 157831\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "  0%|          | 0/970 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/970 [00:02<37:49,  2.34s/it]\u001b[A\n",
      "  0%|          | 2/970 [00:04<36:09,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 3/970 [00:06<35:50,  2.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 4/970 [00:08<35:38,  2.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 5/970 [00:11<35:33,  2.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 6/970 [00:13<35:44,  2.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 7/970 [00:15<35:42,  2.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 8/970 [00:17<35:56,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 9/970 [00:20<35:50,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326240539550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 10/970 [00:22<35:38,  2.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 11/970 [00:24<35:30,  2.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|          | 12/970 [00:26<35:37,  2.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 13/970 [00:29<35:44,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326239585876465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  1%|▏         | 14/970 [00:31<35:55,  2.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326240539550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 15/970 [00:33<36:18,  2.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326240539550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 16/970 [00:35<36:31,  2.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326241493225098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 17/970 [00:38<36:18,  2.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326242446899414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 18/970 [00:40<36:10,  2.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326245307922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 19/970 [00:42<35:57,  2.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326245307922363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 20/970 [00:44<35:31,  2.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.32624626159668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 21/970 [00:47<35:15,  2.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326247215270996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 22/970 [00:49<35:43,  2.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326247215270996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 23/970 [00:51<35:46,  2.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326248168945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▏         | 24/970 [00:54<35:57,  2.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326249122619629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 25/970 [00:56<36:16,  2.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326249122619629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 26/970 [00:58<36:12,  2.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326251029968262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 27/970 [01:00<35:43,  2.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326255798339844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  3%|▎         | 28/970 [01:03<35:54,  2.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.326254844665527\n"
     ]
    }
   ],
   "source": [
    "corpus = clean_text(corpus)\n",
    "corpus = split_on_sequences(corpus)\n",
    "\n",
    "tcorpus = tokenizer.encode_batch(corpus)\n",
    "#tcorpus = tuple(map(lambda sentence: tokenizer.encode(sentence), corpus))\n",
    "\n",
    "## create n-grams for each doc\n",
    "sq = create_ngrams(tcorpus, config.N_GRAM)\n",
    "\n",
    "## shift corpus to create x and y \n",
    "x, y =  create_to_x_and_y(sq)\n",
    "\n",
    "id_token, token_id = create_vocab(tokenizer)\n",
    "vocab_size = len(token_id)\n",
    "\n",
    "# # for passing to dataloader\n",
    "# x_int = [word2int(i, token_id) for i in x]\n",
    "# y_int = [word2int(i, token_id) for i in y]\n",
    "\n",
    "## split data\n",
    "tradeoff_index = int(len(x) * config.TRAIN_PROPORTION)\n",
    "\n",
    "x_train = x[:tradeoff_index]\n",
    "x_test = x[tradeoff_index:]\n",
    "\n",
    "y_train = y[:tradeoff_index]\n",
    "y_test = y[tradeoff_index:]\n",
    "\n",
    "logger.warning(f\"Outpur shapes: x_train: {len(x_train)}, x_test: {len(x_test)}, y_train: {len(y_train)}, y_test: {len(y_test)}\")\n",
    "\n",
    "## load to dataset and dataloader\n",
    "train_ds = LMDataset(x_train, y_train)\n",
    "test_ds = LMDataset(x_test, y_test)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=config.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# model and model config\n",
    "model_config = LanguageModelConfig(vocab_size=vocab_size, emb_size=300)\n",
    "# with open(config.SAVE_MODEL_CONFIG, \"w\") as f: \n",
    "#     json.dump(model_config.dict(), f)\n",
    "model = LM_LSTM(**model_config.dict(), logger=logger)\n",
    "\n",
    "## save artifacts\n",
    "save_artifacts(\n",
    "    (model_config.dict(), config.SAVE_MODEL_CONFIG),\n",
    "    (token_id, config.SAVE_TOKEN_ID),\n",
    "    (id_token, config.SAVE_ID_TOKEN)\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_config.lr)\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# train model \n",
    "tmodel = train_model(model,\n",
    "                     train_dl,\n",
    "                     optimizer=optimizer,\n",
    "                     loss_func=loss_func,\n",
    "                     batch_size=config.BATCH_SIZE,\n",
    "                     epochs=30, \n",
    "                     clip=1)\n",
    "\n",
    "torch.save(model.state_dict(), config.SAVE_MODEL_FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-difficulty",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "upset-hollow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from config import LanguageModelConfig, Config\n",
    "from model import LM_LSTM\n",
    "from loguru import logger\n",
    "from processing_utils import load_artifact\n",
    "\n",
    "## usefull utils\n",
    "logger.remove()\n",
    "logger.add(sys.stderr, level=\"WARNING\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## load artifacts\n",
    "model_config = LanguageModelConfig.parse_file(Config.SAVE_MODEL_CONFIG)\n",
    "token_id = load_artifact(Config.SAVE_TOKEN_ID)\n",
    "id_token = load_artifact(Config.SAVE_ID_TOKEN)\n",
    "\n",
    "## load trained model\n",
    "model = LM_LSTM(**model_config.dict(), logger=logger)\n",
    "model.load_state_dict(\n",
    "    torch.load(Config.SAVE_MODEL_FNAME, map_location=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "confident-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_raw_word(word, token_id):\n",
    "    int_word = token_id.get(word, token_id[\"[UNK]\"])\n",
    "    return torch.tensor([[int_word]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "logical-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BertWordPieceTokenizer in module tokenizers.implementations.bert_wordpiece:\n",
      "\n",
      "class BertWordPieceTokenizer(tokenizers.implementations.base_tokenizer.BaseTokenizer)\n",
      " |  BertWordPieceTokenizer(vocab: Union[str, Dict[str, int], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '[UNK]', sep_token: Union[str, tokenizers.AddedToken] = '[SEP]', cls_token: Union[str, tokenizers.AddedToken] = '[CLS]', pad_token: Union[str, tokenizers.AddedToken] = '[PAD]', mask_token: Union[str, tokenizers.AddedToken] = '[MASK]', clean_text: bool = True, handle_chinese_chars: bool = True, strip_accents: Union[bool, NoneType] = None, lowercase: bool = True, wordpieces_prefix: str = '##')\n",
      " |  \n",
      " |  Bert WordPiece Tokenizer\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BertWordPieceTokenizer\n",
      " |      tokenizers.implementations.base_tokenizer.BaseTokenizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab: Union[str, Dict[str, int], NoneType] = None, unk_token: Union[str, tokenizers.AddedToken] = '[UNK]', sep_token: Union[str, tokenizers.AddedToken] = '[SEP]', cls_token: Union[str, tokenizers.AddedToken] = '[CLS]', pad_token: Union[str, tokenizers.AddedToken] = '[PAD]', mask_token: Union[str, tokenizers.AddedToken] = '[MASK]', clean_text: bool = True, handle_chinese_chars: bool = True, strip_accents: Union[bool, NoneType] = None, lowercase: bool = True, wordpieces_prefix: str = '##')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  train(self, files: Union[str, List[str]], vocab_size: int = 30000, min_frequency: int = 2, limit_alphabet: int = 1000, initial_alphabet: List[str] = [], special_tokens: List[Union[str, tokenizers.AddedToken]] = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'], show_progress: bool = True, wordpieces_prefix: str = '##')\n",
      " |      Train the model using the given files\n",
      " |  \n",
      " |  train_from_iterator(self, iterator: Union[Iterator[str], Iterator[Iterator[str]]], vocab_size: int = 30000, min_frequency: int = 2, limit_alphabet: int = 1000, initial_alphabet: List[str] = [], special_tokens: List[Union[str, tokenizers.AddedToken]] = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'], show_progress: bool = True, wordpieces_prefix: str = '##')\n",
      " |      Train the model using the given iterator\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_file(vocab: str, **kwargs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_special_tokens(self, special_tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add the given special tokens to the vocabulary, and treat them as special tokens.\n",
      " |      \n",
      " |      The special tokens will never be processed by the model, and will be\n",
      " |      removed while decoding.\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: List[Union[str, AddedToken]]:\n",
      " |              A list of special tokens to add to the vocabulary. Each token can either be\n",
      " |              a string, or an instance of AddedToken\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of tokens that were added to the vocabulary\n",
      " |  \n",
      " |  add_tokens(self, tokens: List[Union[str, tokenizers.AddedToken]]) -> int\n",
      " |      Add the given tokens to the vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          tokens: List[Union[str, AddedToken]]:\n",
      " |              A list of tokens to add to the vocabulary. Each token can either be\n",
      " |              a string, or an instance of AddedToken\n",
      " |      \n",
      " |      Returns:\n",
      " |          The number of tokens that were added to the vocabulary\n",
      " |  \n",
      " |  decode(self, ids: List[int], skip_special_tokens: Union[bool, NoneType] = True) -> str\n",
      " |      Decode the given list of ids to a string sequence\n",
      " |      \n",
      " |      Args:\n",
      " |          ids: List[unsigned int]:\n",
      " |              A list of ids to be decoded\n",
      " |      \n",
      " |          skip_special_tokens: (`optional`) boolean:\n",
      " |              Whether to remove all the special tokens from the output string\n",
      " |      \n",
      " |      Returns:\n",
      " |          The decoded string\n",
      " |  \n",
      " |  decode_batch(self, sequences: List[List[int]], skip_special_tokens: Union[bool, NoneType] = True) -> str\n",
      " |      Decode the list of sequences to a list of string sequences\n",
      " |      \n",
      " |      Args:\n",
      " |          sequences: List[List[unsigned int]]:\n",
      " |              A list of sequence of ids to be decoded\n",
      " |      \n",
      " |          skip_special_tokens: (`optional`) boolean:\n",
      " |              Whether to remove all the special tokens from the output strings\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of decoded strings\n",
      " |  \n",
      " |  enable_padding(self, direction: Union[str, NoneType] = 'right', pad_to_multiple_of: Union[int, NoneType] = None, pad_id: Union[int, NoneType] = 0, pad_type_id: Union[int, NoneType] = 0, pad_token: Union[str, NoneType] = '[PAD]', length: Union[int, NoneType] = None)\n",
      " |      Change the padding strategy\n",
      " |      \n",
      " |      Args:\n",
      " |          direction: (`optional`) str:\n",
      " |              Can be one of: `right` or `left`\n",
      " |      \n",
      " |          pad_to_multiple_of: (`optional`) unsigned int:\n",
      " |              If specified, the padding length should always snap to the next multiple of\n",
      " |              the given value. For example if we were going to pad with a length of 250 but\n",
      " |              `pad_to_multiple_of=8` then we will pad to 256.\n",
      " |      \n",
      " |          pad_id: (`optional`) unsigned int:\n",
      " |              The indice to be used when padding\n",
      " |      \n",
      " |          pad_type_id: (`optional`) unsigned int:\n",
      " |              The type indice to be used when padding\n",
      " |      \n",
      " |          pad_token: (`optional`) str:\n",
      " |              The pad token to be used when padding\n",
      " |      \n",
      " |          length: (`optional`) unsigned int:\n",
      " |              If specified, the length at which to pad. If not specified\n",
      " |              we pad using the size of the longest sequence in a batch\n",
      " |  \n",
      " |  enable_truncation(self, max_length: int, stride: Union[int, NoneType] = 0, strategy: Union[str, NoneType] = 'longest_first')\n",
      " |      Change the truncation options\n",
      " |      \n",
      " |      Args:\n",
      " |          max_length: unsigned int:\n",
      " |              The maximum length at which to truncate\n",
      " |      \n",
      " |          stride: (`optional`) unsigned int:\n",
      " |              The length of the previous first sequence to be included\n",
      " |              in the overflowing sequence\n",
      " |      \n",
      " |          strategy: (`optional`) str:\n",
      " |              Can be one of `longest_first`, `only_first` or `only_second`\n",
      " |  \n",
      " |  encode(self, sequence: Union[str, List[str], Tuple[str]], pair: Union[str, List[str], Tuple[str], NoneType] = None, is_pretokenized: bool = False, add_special_tokens: bool = True) -> tokenizers.Encoding\n",
      " |      Encode the given sequence and pair. This method can process raw text sequences as well\n",
      " |      as already pre-tokenized sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          sequence: InputSequence:\n",
      " |              The sequence we want to encode. This sequence can be either raw text or\n",
      " |              pre-tokenized, according to the `is_pretokenized` argument:\n",
      " |      \n",
      " |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n",
      " |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n",
      " |                  `Union[List[str], Tuple[str]]`\n",
      " |      \n",
      " |          is_pretokenized: bool:\n",
      " |              Whether the input is already pre-tokenized.\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add the special tokens while encoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An Encoding\n",
      " |  \n",
      " |  encode_batch(self, inputs: List[Union[str, Tuple[str, str], List[str], Tuple[str], Tuple[Union[List[str], Tuple[str]], Union[List[str], Tuple[str]]], List[Union[List[str], Tuple[str]]]]], is_pretokenized: bool = False, add_special_tokens: bool = True) -> List[tokenizers.Encoding]\n",
      " |      Encode the given inputs. This method accept both raw text sequences as well as already\n",
      " |      pre-tokenized sequences.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: List[EncodeInput]:\n",
      " |              A list of single sequences or pair sequences to encode. Each `EncodeInput` is\n",
      " |              expected to be of the following form:\n",
      " |                  `Union[InputSequence, Tuple[InputSequence, InputSequence]]`\n",
      " |      \n",
      " |              Each `InputSequence` can either be raw text or pre-tokenized,\n",
      " |              according to the `is_pretokenized` argument:\n",
      " |      \n",
      " |              - If `is_pretokenized=False`: `InputSequence` is expected to be `str`\n",
      " |              - If `is_pretokenized=True`: `InputSequence` is expected to be\n",
      " |                  `Union[List[str], Tuple[str]]`\n",
      " |      \n",
      " |          is_pretokenized: bool:\n",
      " |              Whether the input is already pre-tokenized.\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add the special tokens while encoding.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of Encoding\n",
      " |  \n",
      " |  get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]\n",
      " |      Returns the vocabulary\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens: boolean:\n",
      " |              Whether to include the added tokens in the vocabulary\n",
      " |      \n",
      " |      Returns:\n",
      " |          The vocabulary\n",
      " |  \n",
      " |  get_vocab_size(self, with_added_tokens: bool = True) -> int\n",
      " |      Return the size of vocabulary, with or without added tokens.\n",
      " |      \n",
      " |      Args:\n",
      " |          with_added_tokens: (`optional`) bool:\n",
      " |              Whether to count in added special tokens or not\n",
      " |      \n",
      " |      Returns:\n",
      " |          Size of vocabulary\n",
      " |  \n",
      " |  id_to_token(self, id: int) -> Union[str, NoneType]\n",
      " |      Convert the given token id to its corresponding string\n",
      " |      \n",
      " |      Args:\n",
      " |          token: id:\n",
      " |              The token id to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          The corresponding string if it exists, None otherwise\n",
      " |  \n",
      " |  no_padding(self)\n",
      " |      Disable padding\n",
      " |  \n",
      " |  no_truncation(self)\n",
      " |      Disable truncation\n",
      " |  \n",
      " |  normalize(self, sequence: str) -> str\n",
      " |      Normalize the given sequence\n",
      " |      \n",
      " |      Args:\n",
      " |          sequence: str:\n",
      " |              The sequence to normalize\n",
      " |      \n",
      " |      Returns:\n",
      " |          The normalized string\n",
      " |  \n",
      " |  num_special_tokens_to_add(self, is_pair: bool) -> int\n",
      " |      Return the number of special tokens that would be added for single/pair sentences.\n",
      " |      :param is_pair: Boolean indicating if the input would be a single sentence or a pair\n",
      " |      :return:\n",
      " |  \n",
      " |  post_process(self, encoding: tokenizers.Encoding, pair: Union[tokenizers.Encoding, NoneType] = None, add_special_tokens: bool = True) -> tokenizers.Encoding\n",
      " |      Apply all the post-processing steps to the given encodings.\n",
      " |      \n",
      " |      The various steps are:\n",
      " |          1. Truncate according to global params (provided to `enable_truncation`)\n",
      " |          2. Apply the PostProcessor\n",
      " |          3. Pad according to global params. (provided to `enable_padding`)\n",
      " |      \n",
      " |      Args:\n",
      " |          encoding: Encoding:\n",
      " |              The main Encoding to post process\n",
      " |      \n",
      " |          pair: Optional[Encoding]:\n",
      " |              An optional pair Encoding\n",
      " |      \n",
      " |          add_special_tokens: bool:\n",
      " |              Whether to add special tokens\n",
      " |      \n",
      " |      Returns:\n",
      " |          The resulting Encoding\n",
      " |  \n",
      " |  save(self, path: str, pretty: bool = False)\n",
      " |      Save the current Tokenizer at the given path\n",
      " |      \n",
      " |      Args:\n",
      " |          path: str:\n",
      " |              A path to the destination Tokenizer file\n",
      " |  \n",
      " |  save_model(self, directory: str, prefix: Union[str, NoneType] = None)\n",
      " |      Save the current model to the given directory\n",
      " |      \n",
      " |      Args:\n",
      " |          directory: str:\n",
      " |              A path to the destination directory\n",
      " |      \n",
      " |          prefix: (Optional) str:\n",
      " |              An optional prefix, used to prefix each file name\n",
      " |  \n",
      " |  to_str(self, pretty: bool = False)\n",
      " |      Get a serialized JSON version of the Tokenizer as a str\n",
      " |      \n",
      " |      Args:\n",
      " |          pretty: bool:\n",
      " |              Whether the JSON string should be prettified\n",
      " |      \n",
      " |      Returns:\n",
      " |          str\n",
      " |  \n",
      " |  token_to_id(self, token: str) -> Union[int, NoneType]\n",
      " |      Convert the given token to its corresponding id\n",
      " |      \n",
      " |      Args:\n",
      " |          token: str:\n",
      " |              The token to convert\n",
      " |      \n",
      " |      Returns:\n",
      " |          The corresponding id if it exists, None otherwise\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tokenizers.implementations.base_tokenizer.BaseTokenizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  decoder\n",
      " |  \n",
      " |  model\n",
      " |  \n",
      " |  normalizer\n",
      " |  \n",
      " |  padding\n",
      " |      Get the current padding parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          None if padding is disabled, a dict with the currently set parameters\n",
      " |          if the padding is enabled.\n",
      " |  \n",
      " |  post_processor\n",
      " |  \n",
      " |  pre_tokenizer\n",
      " |  \n",
      " |  truncation\n",
      " |      Get the current truncation parameters\n",
      " |      \n",
      " |      Returns:\n",
      " |          None if truncation is disabled, a dict with the current truncation parameters if\n",
      " |          truncation is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "help(BertWordPieceTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "sized-advance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"[UNK]\" in token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "rural-sapphire",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[UNK]'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-d16cf4f57933>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransform_raw_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hell\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-53-47d09b079ea6>\u001b[0m in \u001b[0;36mtransform_raw_word\u001b[0;34m(word, token_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform_raw_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mint_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"[UNK]\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '[UNK]'"
     ]
    }
   ],
   "source": [
    "transform_raw_word(\"hell\", token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fifth-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict next token\n",
    "def predict(net, tkn, h=None):\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[token2int[tkn]]])\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    # push to GPU\n",
    "    inputs = inputs.cuda()\n",
    "\n",
    "    # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "    # get the token probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "\n",
    "    p = p.cpu()\n",
    "\n",
    "    p = p.numpy()\n",
    "    p = p.reshape(p.shape[1],)\n",
    "\n",
    "    # get indices of top 3 values\n",
    "    top_n_idx = p.argsort()[-3:][::-1]\n",
    "\n",
    "    # randomly select one of the three indices\n",
    "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
    "\n",
    "    # return the encoded value of the predicted char and the hidden state\n",
    "    return int2token[sampled_token_index], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "processed-myanmar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to generate text\n",
    "def sample(net, size, prime='it is'):\n",
    "\n",
    "    # push to GPU\n",
    "    net.cuda()\n",
    "\n",
    "    net.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    h = net.init_hidden(1)\n",
    "\n",
    "    toks = prime.split()\n",
    "\n",
    "    # predict next token\n",
    "    for t in prime.split():\n",
    "        token, h = predict(net, t, h)\n",
    "\n",
    "    toks.append(token)\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for i in range(size-1):\n",
    "        token, h = predict(net, toks[-1], h)\n",
    "        toks.append(token)\n",
    "\n",
    "    return ' '.join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-investing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
